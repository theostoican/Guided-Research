{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlMG9xYyz3MIetkIc+8Zgz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7e204e94bc24812b0c757193d751b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d935555f30d84b8db674d9334a46e796",
              "IPY_MODEL_70bd12a0d32c4869a380b6e120debcb9",
              "IPY_MODEL_b9982d805a9f44329b1a5cc5f6cb6b41"
            ],
            "layout": "IPY_MODEL_6a65f8dafb294e3699802679511ba9d3"
          }
        },
        "d935555f30d84b8db674d9334a46e796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d81a7d8c63ba4bb5ad0d03bf5c98c156",
            "placeholder": "​",
            "style": "IPY_MODEL_238817a8c7384658825067e32ecd23fd",
            "value": "100%"
          }
        },
        "70bd12a0d32c4869a380b6e120debcb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f67a8e791104231b7600c0db13efb3d",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9d4d74f819c4827a0d047753bda97a2",
            "value": 46830571
          }
        },
        "b9982d805a9f44329b1a5cc5f6cb6b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cb006f5c14d40e08790ca65b3e43aa2",
            "placeholder": "​",
            "style": "IPY_MODEL_a58021ba29934d20aad4677c6575a421",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 78.4MB/s]"
          }
        },
        "6a65f8dafb294e3699802679511ba9d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81a7d8c63ba4bb5ad0d03bf5c98c156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238817a8c7384658825067e32ecd23fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f67a8e791104231b7600c0db13efb3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d4d74f819c4827a0d047753bda97a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cb006f5c14d40e08790ca65b3e43aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a58021ba29934d20aad4677c6575a421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theostoican/Guided-Research/blob/main/byol_pca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xdhddSdaPKB",
        "outputId": "2ce65716-b16e-44ce-c922-1dbef4236d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting byol_pytorch\n",
            "  Downloading byol_pytorch-0.6.0-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from byol_pytorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.7/dist-packages (from byol_pytorch) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->byol_pytorch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8->byol_pytorch) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8->byol_pytorch) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8->byol_pytorch) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8->byol_pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8->byol_pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8->byol_pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8->byol_pytorch) (2022.6.15)\n",
            "Installing collected packages: byol-pytorch\n",
            "Successfully installed byol-pytorch-0.6.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "!pip install byol_pytorch\n",
        "from byol_pytorch import BYOL\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
        "resnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a7e204e94bc24812b0c757193d751b26",
            "d935555f30d84b8db674d9334a46e796",
            "70bd12a0d32c4869a380b6e120debcb9",
            "b9982d805a9f44329b1a5cc5f6cb6b41",
            "6a65f8dafb294e3699802679511ba9d3",
            "d81a7d8c63ba4bb5ad0d03bf5c98c156",
            "238817a8c7384658825067e32ecd23fd",
            "7f67a8e791104231b7600c0db13efb3d",
            "a9d4d74f819c4827a0d047753bda97a2",
            "1cb006f5c14d40e08790ca65b3e43aa2",
            "a58021ba29934d20aad4677c6575a421"
          ]
        },
        "id": "qYegLYiYaiOm",
        "outputId": "043f1a06-e5f7-475f-ec09-7172dc1ba29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7e204e94bc24812b0c757193d751b26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learner = BYOL(\n",
        "    resnet,\n",
        "    image_size = 256,\n",
        "    hidden_layer = 'avgpool',\n",
        ")\n",
        "\n",
        "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "boxp2p2rbGSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ]) \n",
        "\n",
        "print(os.listdir('data'))\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for filename in os.listdir('data'):\n",
        "  img = Image.open(os.path.join('data', filename))\n",
        "  img = img.convert('RGB')\n",
        "\n",
        "  img = preprocess(img)\n",
        "\n",
        "  dataset.append(img)\n",
        "\n",
        "dataset = torch.stack(dataset).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAVd81DngbfP",
        "outputId": "fb6ab35f-01c1-4f7f-f7af-fe1d5fdd3e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book3.jpg', 'remote17.jpg', 'remote7.jpg', 'baseball3.jpg', 'teddybear16.jpg', 'orange9.jpg', 'cake6.jpg', 'suitcase12.jpg', 'toytrain14.jpg', 'bottle13.jpg', 'ball16.jpg', 'teddybear14.jpg', 'suitcase4.jpg', 'ball9.jpg', 'remote12.jpg', 'donut13.jpg', 'bottle17.jpg', 'broccoli16.jpg', 'toytrain11.jpg', 'bottle10.jpg', 'toytrain12.jpg', 'toytrain15.jpg', 'bottle3.jpg', 'book14.jpg', 'orange2.jpg', 'bottle16.jpg', 'remote4.jpg', 'handbag1.jpg', 'cake3.jpg', 'plant3.jpg', 'teddybear1.jpg', 'donut15.jpg', 'plant5.jpg', 'toytrain17.jpg', 'ball17.jpg', 'book6.jpg', 'toytrain16.jpg', 'remote18.jpg', 'orange17.jpg', 'baseballglove4.jpg', 'teddybear12.jpg', 'apple5.jpg', 'toytrain6.jpg', 'book16.jpg', 'suitcase11.jpg', 'bottle14.jpg', 'suitcase16.jpg', 'teddybear5.jpg', 'skateboard2.jpg', 'backpack5.jpg', 'toytrain18.jpg', 'backpack1.jpg', 'teddybear2.jpg', 'backpack3.jpg', 'baseballglove2.jpg', 'suitcase13.jpg', 'baseball1.jpg', 'broccoli11.jpg', 'frisbee2.jpg', 'orange12.jpg', 'cake14.jpg', 'broccoli12.jpg', 'skateboard5.jpg', 'suitcase7.jpg', 'teddybear9.jpg', 'teddybear11.jpg', 'book1.jpg', 'toytrain7.jpg', 'book10.jpg', 'donut9.jpg', 'cake11.jpg', 'ball2.jpg', 'bottle12.jpg', 'orange16.jpg', 'teddybear17.jpg', 'teddybear18.jpg', 'teddybear13.jpg', 'donut6.jpg', 'broccoli10.jpg', 'suitcase5.jpg', 'ball11.jpg', 'plant4.jpg', 'book18.jpg', 'keyboard4.jpg', 'ball5.jpg', 'broccoli4.jpg', 'ball12.jpg', 'mouse4.jpg', 'keyboard5.jpg', 'mouse5.jpg', 'frisbee4.jpg', 'teddybear15.jpg', 'handbag2.jpg', 'baseball2.jpg', 'baseballglove5.jpg', 'baseball4.jpg', 'ball18.jpg', 'donut1.jpg', 'cake9.jpg', 'remote8.jpg', 'book5.jpg', 'plant1.jpg', 'orange13.jpg', 'mouse2.jpg', 'frisbee3.jpg', 'bottle6.jpg', 'cake17.jpg', 'ball3.jpg', 'book9.jpg', 'ball4.jpg', 'toytrain4.jpg', 'cake8.jpg', 'cake4.jpg', 'orange14.jpg', 'ball13.jpg', 'suitcase14.jpg', 'handbag5.jpg', 'donut16.jpg', 'cake13.jpg', 'teddybear8.jpg', 'suitcase3.jpg', 'book8.jpg', 'remote9.jpg', 'bottle1.jpg', 'baseballglove3.jpg', 'orange18.jpg', 'broccoli8.jpg', 'suitcase9.jpg', 'ball6.jpg', 'toytrain8.jpg', 'frisbee5.jpg', 'teddybear7.jpg', 'book4.jpg', 'suitcase10.jpg', 'keyboard3.jpg', 'apple3.jpg', 'baseball5.jpg', 'bottle15.jpg', 'frisbee1.jpg', 'remote5.jpg', 'donut10.jpg', 'remote15.jpg', 'book17.jpg', 'remote16.jpg', 'book13.jpg', 'teddybear3.jpg', 'orange8.jpg', 'apple6.jpg', 'remote13.jpg', 'teddybear4.jpg', 'donut12.jpg', 'remote14.jpg', 'orange4.jpg', 'suitcase18.jpg', 'donut7.jpg', 'backpack4.jpg', 'book7.jpg', 'orange11.jpg', 'skateboard1.jpg', 'bottle5.jpg', 'broccoli18.jpg', 'suitcase6.jpg', 'book2.jpg', 'suitcase17.jpg', 'broccoli9.jpg', 'bottle9.jpg', 'toytrain1.jpg', 'bottle4.jpg', 'cake7.jpg', 'apple2.jpg', 'skateboard3.jpg', 'cake12.jpg', 'toytrain3.jpg', 'remote1.jpg', 'bottle11.jpg', 'broccoli7.jpg', 'remote10.jpg', 'orange1.jpg', 'orange7.jpg', 'remote6.jpg', 'mouse3.jpg', 'orange3.jpg', 'donut11.jpg', 'ball10.jpg', 'suitcase8.jpg', 'cake15.jpg', 'broccoli17.jpg', 'remote3.jpg', 'broccoli3.jpg', 'donut3.jpg', 'donut8.jpg', 'book11.jpg', 'orange10.jpg', 'ball15.jpg', 'teddybear10.jpg', 'ball14.jpg', 'cake18.jpg', 'cake16.jpg', 'toytrain13.jpg', 'apple4.jpg', 'book15.jpg', 'broccoli1.jpg', 'broccoli15.jpg', 'toytrain10.jpg', 'book12.jpg', 'orange6.jpg', 'suitcase1.jpg', 'keyboard1.jpg', 'cake10.jpg', 'mouse1.jpg', 'remote11.jpg', 'donut5.jpg', 'suitcase2.jpg', 'handbag4.jpg', 'cake2.jpg', 'keyboard2.jpg', 'suitcase15.jpg', 'bottle18.jpg', 'plant2.jpg', 'toytrain2.jpg', 'donut2.jpg', 'cake5.jpg', 'backpack2.jpg', 'bottle8.jpg', 'toytrain5.jpg', 'apple1.jpg', 'broccoli2.jpg', 'bottle7.jpg', 'orange15.jpg', 'orange5.jpg', 'cake1.jpg', 'donut14.jpg', 'teddybear6.jpg', 'ball1.jpg', 'broccoli6.jpg', 'ball7.jpg', 'toytrain9.jpg', 'donut17.jpg', 'handbag3.jpg', 'remote2.jpg', 'donut18.jpg', 'baseballglove1.jpg', 'ball8.jpg', 'bottle2.jpg', 'skateboard4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8FWp7ibmKR-",
        "outputId": "52ae05de-e186-4d58-a6d0-93353b4b2778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([245, 3, 256, 256])\n",
            "245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_size = 64\n",
        "num_batches = np.ceil(len(dataset) / batch_size)\n",
        "\n",
        "resnet.train()\n",
        "\n",
        "for epoch in range(1000):\n",
        "  avg_loss = 0\n",
        "  for batch_idx in np.arange(0, num_batches):\n",
        "    images = dataset[int(batch_idx * batch_size) : int(np.min([(batch_idx + 1) * batch_size, len(dataset)]))]\n",
        "    loss = learner(images)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    avg_loss += loss.item()\n",
        "    learner.update_moving_average() # update moving average of target encoder\n",
        "\n",
        "  avg_loss /= num_batches\n",
        "  print('Epoch ' + str(epoch) + ' and loss: ' + str(avg_loss))\n",
        "    # learner.update_moving_average() # update moving average of target encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JX0r4swhUGH",
        "outputId": "b4fad18a-fd8a-40b1-99ab-b51c0407a890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 and loss: 2.4980663061141968\n",
            "Epoch 1 and loss: 1.4519892930984497\n",
            "Epoch 2 and loss: 1.3825466334819794\n",
            "Epoch 3 and loss: 1.3440146148204803\n",
            "Epoch 4 and loss: 1.205958753824234\n",
            "Epoch 5 and loss: 1.290217563509941\n",
            "Epoch 6 and loss: 1.0783657729625702\n",
            "Epoch 7 and loss: 1.1395358741283417\n",
            "Epoch 8 and loss: 0.9517666548490524\n",
            "Epoch 9 and loss: 0.6368197351694107\n",
            "Epoch 10 and loss: 0.7640950679779053\n",
            "Epoch 11 and loss: 0.581147737801075\n",
            "Epoch 12 and loss: 0.7108235284686089\n",
            "Epoch 13 and loss: 0.6846118196845055\n",
            "Epoch 14 and loss: 0.5933689326047897\n",
            "Epoch 15 and loss: 0.3965212106704712\n",
            "Epoch 16 and loss: 0.246467012912035\n",
            "Epoch 17 and loss: 0.3792143128812313\n",
            "Epoch 18 and loss: 0.17878207191824913\n",
            "Epoch 19 and loss: 0.6906391158699989\n",
            "Epoch 20 and loss: 0.2418738156557083\n",
            "Epoch 21 and loss: 0.33589576929807663\n",
            "Epoch 22 and loss: 0.3607545457780361\n",
            "Epoch 23 and loss: 0.24397265911102295\n",
            "Epoch 24 and loss: 0.15940970741212368\n",
            "Epoch 25 and loss: 0.2974381111562252\n",
            "Epoch 26 and loss: 0.1498761549592018\n",
            "Epoch 27 and loss: 0.28846251778304577\n",
            "Epoch 28 and loss: 0.15238900110125542\n",
            "Epoch 29 and loss: 0.2971028797328472\n",
            "Epoch 30 and loss: 0.11447575315833092\n",
            "Epoch 31 and loss: 0.20863118954002857\n",
            "Epoch 32 and loss: 0.11398045159876347\n",
            "Epoch 33 and loss: 0.09971667360514402\n",
            "Epoch 34 and loss: 0.10135995782911777\n",
            "Epoch 35 and loss: 0.05166623182594776\n",
            "Epoch 36 and loss: 0.12117761466652155\n",
            "Epoch 37 and loss: 0.05615099240094423\n",
            "Epoch 38 and loss: 0.1445527821779251\n",
            "Epoch 39 and loss: 0.10831852443516254\n",
            "Epoch 40 and loss: 0.039361859671771526\n",
            "Epoch 41 and loss: 0.13502714037895203\n",
            "Epoch 42 and loss: 0.11060577351599932\n",
            "Epoch 43 and loss: 0.09670984745025635\n",
            "Epoch 44 and loss: 0.06382022611796856\n",
            "Epoch 45 and loss: 0.09216386172920465\n",
            "Epoch 46 and loss: 0.032765723299235106\n",
            "Epoch 47 and loss: 0.07271262863650918\n",
            "Epoch 48 and loss: 0.038546713069081306\n",
            "Epoch 49 and loss: 0.1346437083557248\n",
            "Epoch 50 and loss: 0.029564365278929472\n",
            "Epoch 51 and loss: 0.08247051946818829\n",
            "Epoch 52 and loss: 0.05346283270046115\n",
            "Epoch 53 and loss: 0.034985174890607595\n",
            "Epoch 54 and loss: 0.11230167839676142\n",
            "Epoch 55 and loss: 0.04279752308502793\n",
            "Epoch 56 and loss: 0.040612560231238604\n",
            "Epoch 57 and loss: 0.04235803568735719\n",
            "Epoch 58 and loss: 0.042354194447398186\n",
            "Epoch 59 and loss: 0.10028526629321277\n",
            "Epoch 60 and loss: 0.21719469083473086\n",
            "Epoch 61 and loss: 0.035528440959751606\n",
            "Epoch 62 and loss: 0.08385261660441756\n",
            "Epoch 63 and loss: 0.02531266724690795\n",
            "Epoch 64 and loss: 0.09403708903118968\n",
            "Epoch 65 and loss: 0.06364494655281305\n",
            "Epoch 66 and loss: 0.04171648947522044\n",
            "Epoch 67 and loss: 0.16377063235267997\n",
            "Epoch 68 and loss: 0.038290027529001236\n",
            "Epoch 69 and loss: 0.04990621004253626\n",
            "Epoch 70 and loss: 0.053262917790561914\n",
            "Epoch 71 and loss: 0.03633303940296173\n",
            "Epoch 72 and loss: 0.028298520017415285\n",
            "Epoch 73 and loss: 0.03831674810498953\n",
            "Epoch 74 and loss: 0.015731009654700756\n",
            "Epoch 75 and loss: 0.031864024233072996\n",
            "Epoch 76 and loss: 0.09054421773180366\n",
            "Epoch 77 and loss: 0.015478703891858459\n",
            "Epoch 78 and loss: 0.028483465313911438\n",
            "Epoch 79 and loss: 0.027080100029706955\n",
            "Epoch 80 and loss: 0.025825271848589182\n",
            "Epoch 81 and loss: 0.0849702968262136\n",
            "Epoch 82 and loss: 0.017923686653375626\n",
            "Epoch 83 and loss: 0.02035545464605093\n",
            "Epoch 84 and loss: 0.04357007169164717\n",
            "Epoch 85 and loss: 0.016614728257991374\n",
            "Epoch 86 and loss: 0.013876697979867458\n",
            "Epoch 87 and loss: 0.01947448868304491\n",
            "Epoch 88 and loss: 0.09658682951703668\n",
            "Epoch 89 and loss: 0.030261408537626266\n",
            "Epoch 90 and loss: 0.04328833846375346\n",
            "Epoch 91 and loss: 0.021249114768579602\n",
            "Epoch 92 and loss: 0.028686583042144775\n",
            "Epoch 93 and loss: 0.05861922702752054\n",
            "Epoch 94 and loss: 0.03838201053440571\n",
            "Epoch 95 and loss: 0.058022588840685785\n",
            "Epoch 96 and loss: 0.041136959567666054\n",
            "Epoch 97 and loss: 0.012479155790060759\n",
            "Epoch 98 and loss: 0.014196475269272923\n",
            "Epoch 99 and loss: 0.010380604770034552\n",
            "Epoch 100 and loss: 0.03167127619963139\n",
            "Epoch 101 and loss: 0.009773066034540534\n",
            "Epoch 102 and loss: 0.03467195783741772\n",
            "Epoch 103 and loss: 0.026056231930851936\n",
            "Epoch 104 and loss: 0.026690262835472822\n",
            "Epoch 105 and loss: 0.03984263981692493\n",
            "Epoch 106 and loss: 0.024795205681584775\n",
            "Epoch 107 and loss: 0.015148283448070288\n",
            "Epoch 108 and loss: 0.03737131739035249\n",
            "Epoch 109 and loss: 0.010197426658123732\n",
            "Epoch 110 and loss: 0.007285998901352286\n",
            "Epoch 111 and loss: 0.03935024421662092\n",
            "Epoch 112 and loss: 0.035080255940556526\n",
            "Epoch 113 and loss: 0.016682330518960953\n",
            "Epoch 114 and loss: 0.01066868589259684\n",
            "Epoch 115 and loss: 0.01753313932567835\n",
            "Epoch 116 and loss: 0.012264430755749345\n",
            "Epoch 117 and loss: 0.19070647691842169\n",
            "Epoch 118 and loss: 0.013671704102307558\n",
            "Epoch 119 and loss: 0.023945619352161884\n",
            "Epoch 120 and loss: 0.033043733797967434\n",
            "Epoch 121 and loss: 0.042864645598456264\n",
            "Epoch 122 and loss: 0.03851234260946512\n",
            "Epoch 123 and loss: 0.01601925538852811\n",
            "Epoch 124 and loss: 0.06629087496548891\n",
            "Epoch 125 and loss: 0.014933322090655565\n",
            "Epoch 126 and loss: 0.027582907117903233\n",
            "Epoch 127 and loss: 0.12780723720788956\n",
            "Epoch 128 and loss: 0.01742349285632372\n",
            "Epoch 129 and loss: 0.03219970595091581\n",
            "Epoch 130 and loss: 0.03589120227843523\n",
            "Epoch 131 and loss: 0.04073215578682721\n",
            "Epoch 132 and loss: 0.07768002431839705\n",
            "Epoch 133 and loss: 0.02063235081732273\n",
            "Epoch 134 and loss: 0.018888555001467466\n",
            "Epoch 135 and loss: 0.023615400306880474\n",
            "Epoch 136 and loss: 0.15978939551860094\n",
            "Epoch 137 and loss: 0.2374255124013871\n",
            "Epoch 138 and loss: 0.06345599237829447\n",
            "Epoch 139 and loss: 0.5304187871515751\n",
            "Epoch 140 and loss: 0.340850418433547\n",
            "Epoch 141 and loss: 0.19237376563251019\n",
            "Epoch 142 and loss: 0.09448474273085594\n",
            "Epoch 143 and loss: 0.11843664012849331\n",
            "Epoch 144 and loss: 0.24024047516286373\n",
            "Epoch 145 and loss: 0.05408942699432373\n",
            "Epoch 146 and loss: 0.062165189534425735\n",
            "Epoch 147 and loss: 0.02574744587764144\n",
            "Epoch 148 and loss: 0.06091940077021718\n",
            "Epoch 149 and loss: 0.06952703651040792\n",
            "Epoch 150 and loss: 0.025027156341820955\n",
            "Epoch 151 and loss: 0.05602703057229519\n",
            "Epoch 152 and loss: 0.04179913783445954\n",
            "Epoch 153 and loss: 0.015053230570629239\n",
            "Epoch 154 and loss: 0.07363066170364618\n",
            "Epoch 155 and loss: 0.08815509942360222\n",
            "Epoch 156 and loss: 0.036500164307653904\n",
            "Epoch 157 and loss: 0.024302704725414515\n",
            "Epoch 158 and loss: 0.013345885090529919\n",
            "Epoch 159 and loss: 0.010492521105334163\n",
            "Epoch 160 and loss: 0.019449954852461815\n",
            "Epoch 161 and loss: 0.011449186597019434\n",
            "Epoch 162 and loss: 0.1417912777978927\n",
            "Epoch 163 and loss: 0.053821835201233625\n",
            "Epoch 164 and loss: 0.016552165150642395\n",
            "Epoch 165 and loss: 0.024158517131581903\n",
            "Epoch 166 and loss: 0.042163274716585875\n",
            "Epoch 167 and loss: 0.01958012394607067\n",
            "Epoch 168 and loss: 0.193514505866915\n",
            "Epoch 169 and loss: 0.019018230959773064\n",
            "Epoch 170 and loss: 0.035658312030136585\n",
            "Epoch 171 and loss: 0.03297588648274541\n",
            "Epoch 172 and loss: 0.1357439775019884\n",
            "Epoch 173 and loss: 0.04741864814423025\n",
            "Epoch 174 and loss: 0.0428311419673264\n",
            "Epoch 175 and loss: 0.08139439532533288\n",
            "Epoch 176 and loss: 0.019569867523387074\n",
            "Epoch 177 and loss: 0.057892824057489634\n",
            "Epoch 178 and loss: 0.05601754831150174\n",
            "Epoch 179 and loss: 0.030360964126884937\n",
            "Epoch 180 and loss: 0.03183459956198931\n",
            "Epoch 181 and loss: 0.021795202046632767\n",
            "Epoch 182 and loss: 0.02065501268953085\n",
            "Epoch 183 and loss: 0.01324129686690867\n",
            "Epoch 184 and loss: 0.13050196878612041\n",
            "Epoch 185 and loss: 0.012315781088545918\n",
            "Epoch 186 and loss: 0.05643312959000468\n",
            "Epoch 187 and loss: 0.008874995866790414\n",
            "Epoch 188 and loss: 0.02436887053772807\n",
            "Epoch 189 and loss: 0.019645155174657702\n",
            "Epoch 190 and loss: 0.008117788704112172\n",
            "Epoch 191 and loss: 0.10190885560587049\n",
            "Epoch 192 and loss: 0.01979751419275999\n",
            "Epoch 193 and loss: 0.09027648041956127\n",
            "Epoch 194 and loss: 0.017608074704185128\n",
            "Epoch 195 and loss: 0.03298379713669419\n",
            "Epoch 196 and loss: 0.04930717102251947\n",
            "Epoch 197 and loss: 0.3739208460319787\n",
            "Epoch 198 and loss: 0.06079588597640395\n",
            "Epoch 199 and loss: 0.06188756972551346\n",
            "Epoch 200 and loss: 0.022007434628903866\n",
            "Epoch 201 and loss: 0.1830284553579986\n",
            "Epoch 202 and loss: 0.1725535374134779\n",
            "Epoch 203 and loss: 0.04102983232587576\n",
            "Epoch 204 and loss: 0.05093432543799281\n",
            "Epoch 205 and loss: 0.025500244926661253\n",
            "Epoch 206 and loss: 0.05315098073333502\n",
            "Epoch 207 and loss: 0.08115138486027718\n",
            "Epoch 208 and loss: 0.026895090006291866\n",
            "Epoch 209 and loss: 0.07619318133220077\n",
            "Epoch 210 and loss: 0.022276525851339102\n",
            "Epoch 211 and loss: 0.01436137338168919\n",
            "Epoch 212 and loss: 0.04226157767698169\n",
            "Epoch 213 and loss: 0.007171798264607787\n",
            "Epoch 214 and loss: 0.030047831940464675\n",
            "Epoch 215 and loss: 0.025036880746483803\n",
            "Epoch 216 and loss: 0.03924207354430109\n",
            "Epoch 217 and loss: 0.07272222684696317\n",
            "Epoch 218 and loss: 0.038397627184167504\n",
            "Epoch 219 and loss: 0.16065483284182847\n",
            "Epoch 220 and loss: 0.013487014919519424\n",
            "Epoch 221 and loss: 0.022685533156618476\n",
            "Epoch 222 and loss: 0.009539728984236717\n",
            "Epoch 223 and loss: 0.01284333923831582\n",
            "Epoch 224 and loss: 0.019294063560664654\n",
            "Epoch 225 and loss: 0.0058834965457208455\n",
            "Epoch 226 and loss: 0.00837884598877281\n",
            "Epoch 227 and loss: 0.014315902953967452\n",
            "Epoch 228 and loss: 0.05830249562859535\n",
            "Epoch 229 and loss: 0.02167315932456404\n",
            "Epoch 230 and loss: 0.020020981202833354\n",
            "Epoch 231 and loss: 0.009498478611931205\n",
            "Epoch 232 and loss: 0.05629428196698427\n",
            "Epoch 233 and loss: 0.04350915551185608\n",
            "Epoch 234 and loss: 0.015769598656333983\n",
            "Epoch 235 and loss: 0.019043427892029285\n",
            "Epoch 236 and loss: 0.02315230038948357\n",
            "Epoch 237 and loss: 0.05424620839767158\n",
            "Epoch 238 and loss: 0.013435215223580599\n",
            "Epoch 239 and loss: 0.015415830130223185\n",
            "Epoch 240 and loss: 0.022625047247856855\n",
            "Epoch 241 and loss: 0.029928733478300273\n",
            "Epoch 242 and loss: 0.022293920279480517\n",
            "Epoch 243 and loss: 0.0076814269996248186\n",
            "Epoch 244 and loss: 0.01901267026551068\n",
            "Epoch 245 and loss: 0.006699632271192968\n",
            "Epoch 246 and loss: 0.012921216897666454\n",
            "Epoch 247 and loss: 0.045373106840997934\n",
            "Epoch 248 and loss: 0.03011239494662732\n",
            "Epoch 249 and loss: 0.008755400776863098\n",
            "Epoch 250 and loss: 0.00850222771987319\n",
            "Epoch 251 and loss: 0.011425878736190498\n",
            "Epoch 252 and loss: 0.031429033260792494\n",
            "Epoch 253 and loss: 0.006521255476400256\n",
            "Epoch 254 and loss: 0.02009312156587839\n",
            "Epoch 255 and loss: 0.010449847439303994\n",
            "Epoch 256 and loss: 0.005962037714198232\n",
            "Epoch 257 and loss: 0.017849392141215503\n",
            "Epoch 258 and loss: 0.011801480315625668\n",
            "Epoch 259 and loss: 0.0060954170767217875\n",
            "Epoch 260 and loss: 0.003411834826692939\n",
            "Epoch 261 and loss: 0.006093691510614008\n",
            "Epoch 262 and loss: 0.09218124393373728\n",
            "Epoch 263 and loss: 0.021174185443669558\n",
            "Epoch 264 and loss: 0.005930898943915963\n",
            "Epoch 265 and loss: 0.017858131555840373\n",
            "Epoch 266 and loss: 0.018899557646363974\n",
            "Epoch 267 and loss: 0.05477874865755439\n",
            "Epoch 268 and loss: 0.008506271871738136\n",
            "Epoch 269 and loss: 0.009229563525877893\n",
            "Epoch 270 and loss: 0.10809792717918754\n",
            "Epoch 271 and loss: 0.009365434292703867\n",
            "Epoch 272 and loss: 0.012000259943306446\n",
            "Epoch 273 and loss: 0.014261096250265837\n",
            "Epoch 274 and loss: 0.009796327911317348\n",
            "Epoch 275 and loss: 0.15292959287762642\n",
            "Epoch 276 and loss: 0.018775527365505695\n",
            "Epoch 277 and loss: 0.10225542401894927\n",
            "Epoch 278 and loss: 0.015537170693278313\n",
            "Epoch 279 and loss: 0.028431127779185772\n",
            "Epoch 280 and loss: 0.011944526806473732\n",
            "Epoch 281 and loss: 0.018764305394142866\n",
            "Epoch 282 and loss: 0.010658743674866855\n",
            "Epoch 283 and loss: 0.014007674064487219\n",
            "Epoch 284 and loss: 0.038057507481426\n",
            "Epoch 285 and loss: 0.009058668045327067\n",
            "Epoch 286 and loss: 0.017767404671758413\n",
            "Epoch 287 and loss: 0.007063896395266056\n",
            "Epoch 288 and loss: 0.03575381822884083\n",
            "Epoch 289 and loss: 0.01878446084447205\n",
            "Epoch 290 and loss: 0.008346089627593756\n",
            "Epoch 291 and loss: 0.01907166640739888\n",
            "Epoch 292 and loss: 0.012029283097945154\n",
            "Epoch 293 and loss: 0.0053305046167224646\n",
            "Epoch 294 and loss: 0.0045784946996718645\n",
            "Epoch 295 and loss: 0.03768338914960623\n",
            "Epoch 296 and loss: 0.010282792616635561\n",
            "Epoch 297 and loss: 0.05870489543303847\n",
            "Epoch 298 and loss: 0.04071584716439247\n",
            "Epoch 299 and loss: 0.019396256655454636\n",
            "Epoch 300 and loss: 0.03629120858386159\n",
            "Epoch 301 and loss: 0.005601541604846716\n",
            "Epoch 302 and loss: 0.018426040071062744\n",
            "Epoch 303 and loss: 0.0069762051571160555\n",
            "Epoch 304 and loss: 0.011441849870607257\n",
            "Epoch 305 and loss: 0.02144114242400974\n",
            "Epoch 306 and loss: 0.0061749202432110906\n",
            "Epoch 307 and loss: 0.004368895490188152\n",
            "Epoch 308 and loss: 0.008499803487211466\n",
            "Epoch 309 and loss: 0.006242793053388596\n",
            "Epoch 310 and loss: 0.006484978133812547\n",
            "Epoch 311 and loss: 0.006570539902895689\n",
            "Epoch 312 and loss: 0.004957544035278261\n",
            "Epoch 313 and loss: 0.011185182956978679\n",
            "Epoch 314 and loss: 0.007462202571332455\n",
            "Epoch 315 and loss: 0.00762800220400095\n",
            "Epoch 316 and loss: 0.0347534017637372\n",
            "Epoch 317 and loss: 0.006518624490126967\n",
            "Epoch 318 and loss: 0.006906964583322406\n",
            "Epoch 319 and loss: 0.005691158236004412\n",
            "Epoch 320 and loss: 0.00796155899297446\n",
            "Epoch 321 and loss: 0.09884419350419194\n",
            "Epoch 322 and loss: 0.007035714574158192\n",
            "Epoch 323 and loss: 0.03310337581206113\n",
            "Epoch 324 and loss: 0.007045585662126541\n",
            "Epoch 325 and loss: 0.006649849005043507\n",
            "Epoch 326 and loss: 0.011326313950121403\n",
            "Epoch 327 and loss: 0.011995656532235444\n",
            "Epoch 328 and loss: 0.0045827553840354085\n",
            "Epoch 329 and loss: 0.00886815832927823\n",
            "Epoch 330 and loss: 0.0045626877108588815\n",
            "Epoch 331 and loss: 0.00711823021993041\n",
            "Epoch 332 and loss: 0.044822053983807564\n",
            "Epoch 333 and loss: 0.0043243191903457046\n",
            "Epoch 334 and loss: 0.014931532903574407\n",
            "Epoch 335 and loss: 0.05876566609367728\n",
            "Epoch 336 and loss: 0.005409201956354082\n",
            "Epoch 337 and loss: 0.006902999128215015\n",
            "Epoch 338 and loss: 0.020002563192974776\n",
            "Epoch 339 and loss: 0.050300049828365445\n",
            "Epoch 340 and loss: 0.034329062909819186\n",
            "Epoch 341 and loss: 0.006216318812221289\n",
            "Epoch 342 and loss: 0.26966154109686613\n",
            "Epoch 343 and loss: 0.014062301721423864\n",
            "Epoch 344 and loss: 0.018380334600806236\n",
            "Epoch 345 and loss: 0.06543826498091221\n",
            "Epoch 346 and loss: 0.06032567750662565\n",
            "Epoch 347 and loss: 0.03399541764520109\n",
            "Epoch 348 and loss: 0.034160865005105734\n",
            "Epoch 349 and loss: 0.02517080819234252\n",
            "Epoch 350 and loss: 0.0505150614771992\n",
            "Epoch 351 and loss: 0.10663419659249485\n",
            "Epoch 352 and loss: 0.04260703828185797\n",
            "Epoch 353 and loss: 0.0188768757507205\n",
            "Epoch 354 and loss: 0.015127973398193717\n",
            "Epoch 355 and loss: 0.01095173298381269\n",
            "Epoch 356 and loss: 0.015183698735199869\n",
            "Epoch 357 and loss: 0.06265030056238174\n",
            "Epoch 358 and loss: 0.03803677624091506\n",
            "Epoch 359 and loss: 0.008564380928874016\n",
            "Epoch 360 and loss: 0.03076494624838233\n",
            "Epoch 361 and loss: 0.008646728936582804\n",
            "Epoch 362 and loss: 0.008090447634458542\n",
            "Epoch 363 and loss: 0.05246338091092184\n",
            "Epoch 364 and loss: 0.0063278721063397825\n",
            "Epoch 365 and loss: 0.0049688927829265594\n",
            "Epoch 366 and loss: 0.026665347162634134\n",
            "Epoch 367 and loss: 0.005896218819543719\n",
            "Epoch 368 and loss: 0.01669118693098426\n",
            "Epoch 369 and loss: 0.006980666192248464\n",
            "Epoch 370 and loss: 0.00870829971972853\n",
            "Epoch 371 and loss: 0.011256549856625497\n",
            "Epoch 372 and loss: 0.010626875096932054\n",
            "Epoch 373 and loss: 0.005025028483942151\n",
            "Epoch 374 and loss: 0.0060043600387871265\n",
            "Epoch 375 and loss: 0.00991632422665134\n",
            "Epoch 376 and loss: 0.0076753131579607725\n",
            "Epoch 377 and loss: 0.009663613920565695\n",
            "Epoch 378 and loss: 0.005075851920992136\n",
            "Epoch 379 and loss: 0.014498201024252921\n",
            "Epoch 380 and loss: 0.010968830902129412\n",
            "Epoch 381 and loss: 0.029932856210507452\n",
            "Epoch 382 and loss: 0.0082188555970788\n",
            "Epoch 383 and loss: 0.0144355408847332\n",
            "Epoch 384 and loss: 0.037780911778099835\n",
            "Epoch 385 and loss: 0.00477602519094944\n",
            "Epoch 386 and loss: 0.008206723490729928\n",
            "Epoch 387 and loss: 0.007858265307731926\n",
            "Epoch 388 and loss: 0.0077668779995292425\n",
            "Epoch 389 and loss: 0.005401307251304388\n",
            "Epoch 390 and loss: 0.005334181769285351\n",
            "Epoch 391 and loss: 0.017787429154850543\n",
            "Epoch 392 and loss: 0.012371016142424196\n",
            "Epoch 393 and loss: 0.02135057095438242\n",
            "Epoch 394 and loss: 0.00533857784466818\n",
            "Epoch 395 and loss: 0.004685731953941286\n",
            "Epoch 396 and loss: 0.007094422704540193\n",
            "Epoch 397 and loss: 0.0022083243529777974\n",
            "Epoch 398 and loss: 0.004476869129575789\n",
            "Epoch 399 and loss: 0.003095795284025371\n",
            "Epoch 400 and loss: 0.0031949226977303624\n",
            "Epoch 401 and loss: 0.004289154545404017\n",
            "Epoch 402 and loss: 0.0032340818434022367\n",
            "Epoch 403 and loss: 0.003165701898979023\n",
            "Epoch 404 and loss: 0.009790322743356228\n",
            "Epoch 405 and loss: 0.00543860555626452\n",
            "Epoch 406 and loss: 0.010315020103007555\n",
            "Epoch 407 and loss: 0.011072274879552424\n",
            "Epoch 408 and loss: 0.004333368910010904\n",
            "Epoch 409 and loss: 0.0025178223149850965\n",
            "Epoch 410 and loss: 0.0027153276605531573\n",
            "Epoch 411 and loss: 0.015814709709957242\n",
            "Epoch 412 and loss: 0.003770874231122434\n",
            "Epoch 413 and loss: 0.003999405656941235\n",
            "Epoch 414 and loss: 0.01907800481421873\n",
            "Epoch 415 and loss: 0.004600196611136198\n",
            "Epoch 416 and loss: 0.005393458413891494\n",
            "Epoch 417 and loss: 0.006260830559767783\n",
            "Epoch 418 and loss: 0.008240348426625133\n",
            "Epoch 419 and loss: 0.03812286866013892\n",
            "Epoch 420 and loss: 0.007562681217677891\n",
            "Epoch 421 and loss: 0.006923891021870077\n",
            "Epoch 422 and loss: 0.00948718358995393\n",
            "Epoch 423 and loss: 0.012150073889642954\n",
            "Epoch 424 and loss: 0.008203119039535522\n",
            "Epoch 425 and loss: 0.04281896841712296\n",
            "Epoch 426 and loss: 0.003886953927576542\n",
            "Epoch 427 and loss: 0.02857177285477519\n",
            "Epoch 428 and loss: 0.03367351903580129\n",
            "Epoch 429 and loss: 0.003584698832128197\n",
            "Epoch 430 and loss: 0.010234877350740135\n",
            "Epoch 431 and loss: 0.007400077069178224\n",
            "Epoch 432 and loss: 0.004362931242212653\n",
            "Epoch 433 and loss: 0.0063692961120978\n",
            "Epoch 434 and loss: 0.006286885007284582\n",
            "Epoch 435 and loss: 0.004243840143317357\n",
            "Epoch 436 and loss: 0.007130890997359529\n",
            "Epoch 437 and loss: 0.005469795782119036\n",
            "Epoch 438 and loss: 0.020799671299755573\n",
            "Epoch 439 and loss: 0.019256083061918616\n",
            "Epoch 440 and loss: 0.003242268576286733\n",
            "Epoch 441 and loss: 0.0049314661882817745\n",
            "Epoch 442 and loss: 0.004001023422461003\n",
            "Epoch 443 and loss: 0.01145655510481447\n",
            "Epoch 444 and loss: 0.008243253570981324\n",
            "Epoch 445 and loss: 0.22716703719925135\n",
            "Epoch 446 and loss: 0.1793686100281775\n",
            "Epoch 447 and loss: 0.024692741222679615\n",
            "Epoch 448 and loss: 0.019169890321791172\n",
            "Epoch 449 and loss: 0.1647807490080595\n",
            "Epoch 450 and loss: 0.017029470996931195\n",
            "Epoch 451 and loss: 0.06970562296919525\n",
            "Epoch 452 and loss: 0.057840810157358646\n",
            "Epoch 453 and loss: 0.027066714596003294\n",
            "Epoch 454 and loss: 0.020683568669483066\n",
            "Epoch 455 and loss: 0.01781361596658826\n",
            "Epoch 456 and loss: 0.07485887361690402\n",
            "Epoch 457 and loss: 0.012698040809482336\n",
            "Epoch 458 and loss: 0.0179169699549675\n",
            "Epoch 459 and loss: 0.03761571645736694\n",
            "Epoch 460 and loss: 0.006876787054352462\n",
            "Epoch 461 and loss: 0.012925867340527475\n",
            "Epoch 462 and loss: 0.01066446932964027\n",
            "Epoch 463 and loss: 0.007739049382507801\n",
            "Epoch 464 and loss: 0.01234115893021226\n",
            "Epoch 465 and loss: 0.011558522935956717\n",
            "Epoch 466 and loss: 0.005985154421068728\n",
            "Epoch 467 and loss: 0.014319730922579765\n",
            "Epoch 468 and loss: 0.04542360833147541\n",
            "Epoch 469 and loss: 0.037337512825615704\n",
            "Epoch 470 and loss: 0.0068528251722455025\n",
            "Epoch 471 and loss: 0.11946891131810844\n",
            "Epoch 472 and loss: 0.05671157711185515\n",
            "Epoch 473 and loss: 0.014749316149391234\n",
            "Epoch 474 and loss: 0.010738916462287307\n",
            "Epoch 475 and loss: 0.008720096084289253\n",
            "Epoch 476 and loss: 0.013966093771159649\n",
            "Epoch 477 and loss: 0.06979090115055442\n",
            "Epoch 478 and loss: 0.015624967752955854\n",
            "Epoch 479 and loss: 0.014690027106553316\n",
            "Epoch 480 and loss: 0.023096534190699458\n",
            "Epoch 481 and loss: 0.05296150594949722\n",
            "Epoch 482 and loss: 0.007908021099865437\n",
            "Epoch 483 and loss: 0.025435384130105376\n",
            "Epoch 484 and loss: 0.006855028332211077\n",
            "Epoch 485 and loss: 0.011018979363143444\n",
            "Epoch 486 and loss: 0.019660146441310644\n",
            "Epoch 487 and loss: 0.024847316439263523\n",
            "Epoch 488 and loss: 0.07384423818439245\n",
            "Epoch 489 and loss: 0.013634815462864935\n",
            "Epoch 490 and loss: 0.033411143347620964\n",
            "Epoch 491 and loss: 0.007495208992622793\n",
            "Epoch 492 and loss: 0.009129739017225802\n",
            "Epoch 493 and loss: 0.05926453857682645\n",
            "Epoch 494 and loss: 0.017794873099774122\n",
            "Epoch 495 and loss: 0.00905127264559269\n",
            "Epoch 496 and loss: 0.01247404352761805\n",
            "Epoch 497 and loss: 0.014105352573096752\n",
            "Epoch 498 and loss: 0.009643892757594585\n",
            "Epoch 499 and loss: 0.01770999887958169\n",
            "Epoch 500 and loss: 0.007313841953873634\n",
            "Epoch 501 and loss: 0.01795039011631161\n",
            "Epoch 502 and loss: 0.08000830200035125\n",
            "Epoch 503 and loss: 0.029120368068106472\n",
            "Epoch 504 and loss: 0.037418343243189156\n",
            "Epoch 505 and loss: 0.010766798164695501\n",
            "Epoch 506 and loss: 0.015520988963544369\n",
            "Epoch 507 and loss: 0.052503918996080756\n",
            "Epoch 508 and loss: 0.06117293785791844\n",
            "Epoch 509 and loss: 0.08562561543658376\n",
            "Epoch 510 and loss: 0.029768225271254778\n",
            "Epoch 511 and loss: 0.15852207667194307\n",
            "Epoch 512 and loss: 0.049944252241402864\n",
            "Epoch 513 and loss: 0.01588309369981289\n",
            "Epoch 514 and loss: 0.019974969793111086\n",
            "Epoch 515 and loss: 0.0422267212998122\n",
            "Epoch 516 and loss: 0.02718375902622938\n",
            "Epoch 517 and loss: 0.2372613693587482\n",
            "Epoch 518 and loss: 0.02098621381446719\n",
            "Epoch 519 and loss: 0.07073304988443851\n",
            "Epoch 520 and loss: 0.0448484206572175\n",
            "Epoch 521 and loss: 0.050272919703274965\n",
            "Epoch 522 and loss: 0.02545852353796363\n",
            "Epoch 523 and loss: 0.06611923174932599\n",
            "Epoch 524 and loss: 0.045664470409974456\n",
            "Epoch 525 and loss: 0.04500512406229973\n",
            "Epoch 526 and loss: 0.07659288076683879\n",
            "Epoch 527 and loss: 0.025908875977620482\n",
            "Epoch 528 and loss: 0.009974842425435781\n",
            "Epoch 529 and loss: 0.055406392086297274\n",
            "Epoch 530 and loss: 0.01242836145684123\n",
            "Epoch 531 and loss: 0.015493713552132249\n",
            "Epoch 532 and loss: 0.029540540650486946\n",
            "Epoch 533 and loss: 0.015580774284899235\n",
            "Epoch 534 and loss: 0.00690137199126184\n",
            "Epoch 535 and loss: 0.009510853793472052\n",
            "Epoch 536 and loss: 0.009341512341052294\n",
            "Epoch 537 and loss: 0.020033943932503462\n",
            "Epoch 538 and loss: 0.2515225540846586\n",
            "Epoch 539 and loss: 0.09948908817023039\n",
            "Epoch 540 and loss: 0.01145072071813047\n",
            "Epoch 541 and loss: 0.009955775574781\n",
            "Epoch 542 and loss: 0.010344400070607662\n",
            "Epoch 543 and loss: 0.012246896978467703\n",
            "Epoch 544 and loss: 0.015567888738587499\n",
            "Epoch 545 and loss: 0.07751028041820973\n",
            "Epoch 546 and loss: 0.00690804363694042\n",
            "Epoch 547 and loss: 0.02079933462664485\n",
            "Epoch 548 and loss: 0.02593190362676978\n",
            "Epoch 549 and loss: 0.0738240871578455\n",
            "Epoch 550 and loss: 0.04910743050277233\n",
            "Epoch 551 and loss: 0.00756813632324338\n",
            "Epoch 552 and loss: 0.017226886935532093\n",
            "Epoch 553 and loss: 0.02938170195557177\n",
            "Epoch 554 and loss: 0.01130697107873857\n",
            "Epoch 555 and loss: 0.00891958677675575\n",
            "Epoch 556 and loss: 0.01594416087027639\n",
            "Epoch 557 and loss: 0.009744013659656048\n",
            "Epoch 558 and loss: 0.008982981089502573\n",
            "Epoch 559 and loss: 0.005536688142456114\n",
            "Epoch 560 and loss: 0.010348937008529902\n",
            "Epoch 561 and loss: 0.003292011038865894\n",
            "Epoch 562 and loss: 0.01694748579757288\n",
            "Epoch 563 and loss: 0.015087340958416462\n",
            "Epoch 564 and loss: 0.035877338086720556\n",
            "Epoch 565 and loss: 0.005056835128925741\n",
            "Epoch 566 and loss: 0.005338784074410796\n",
            "Epoch 567 and loss: 0.007698713365243748\n",
            "Epoch 568 and loss: 0.0029323925264179707\n",
            "Epoch 569 and loss: 0.025707474211230874\n",
            "Epoch 570 and loss: 0.004008045536465943\n",
            "Epoch 571 and loss: 0.004142932244576514\n",
            "Epoch 572 and loss: 0.07898694777395576\n",
            "Epoch 573 and loss: 0.009211475495249033\n",
            "Epoch 574 and loss: 0.00590819789795205\n",
            "Epoch 575 and loss: 0.024992287158966064\n",
            "Epoch 576 and loss: 0.0034330658963881433\n",
            "Epoch 577 and loss: 0.030109497252851725\n",
            "Epoch 578 and loss: 0.004379068035632372\n",
            "Epoch 579 and loss: 0.00545869953930378\n",
            "Epoch 580 and loss: 0.011170680343639106\n",
            "Epoch 581 and loss: 0.1423233118839562\n",
            "Epoch 582 and loss: 0.009505080408416688\n",
            "Epoch 583 and loss: 0.011950840591453016\n",
            "Epoch 584 and loss: 0.019719277508556843\n",
            "Epoch 585 and loss: 0.01130331214517355\n",
            "Epoch 586 and loss: 0.006638266495428979\n",
            "Epoch 587 and loss: 0.00642787414835766\n",
            "Epoch 588 and loss: 0.003359949856530875\n",
            "Epoch 589 and loss: 0.0060355872847139835\n",
            "Epoch 590 and loss: 0.0033963373862206936\n",
            "Epoch 591 and loss: 0.011423425399698317\n",
            "Epoch 592 and loss: 0.008138238452374935\n",
            "Epoch 593 and loss: 0.03239385550841689\n",
            "Epoch 594 and loss: 0.03869801666587591\n",
            "Epoch 595 and loss: 0.04809684364590794\n",
            "Epoch 596 and loss: 0.0198230785317719\n",
            "Epoch 597 and loss: 0.008308393182232976\n",
            "Epoch 598 and loss: 0.01994119375012815\n",
            "Epoch 599 and loss: 0.05322382692247629\n",
            "Epoch 600 and loss: 0.040450576692819595\n",
            "Epoch 601 and loss: 0.03196083940565586\n",
            "Epoch 602 and loss: 0.018509060028009117\n",
            "Epoch 603 and loss: 0.08206949604209512\n",
            "Epoch 604 and loss: 0.07614911254495382\n",
            "Epoch 605 and loss: 0.024544278159737587\n",
            "Epoch 606 and loss: 0.010556272463873029\n",
            "Epoch 607 and loss: 0.007557259756140411\n",
            "Epoch 608 and loss: 0.01080236118286848\n",
            "Epoch 609 and loss: 0.005049640370998532\n",
            "Epoch 610 and loss: 0.02981609315611422\n",
            "Epoch 611 and loss: 0.012262021075002849\n",
            "Epoch 612 and loss: 0.004660930950194597\n",
            "Epoch 613 and loss: 0.029740792117081583\n",
            "Epoch 614 and loss: 0.007323881844058633\n",
            "Epoch 615 and loss: 0.051310133654624224\n",
            "Epoch 616 and loss: 0.007179681095294654\n",
            "Epoch 617 and loss: 0.028184114024043083\n",
            "Epoch 618 and loss: 0.01656974828802049\n",
            "Epoch 619 and loss: 0.060493449214845896\n",
            "Epoch 620 and loss: 0.008815224980935454\n",
            "Epoch 621 and loss: 0.005466661648824811\n",
            "Epoch 622 and loss: 0.0238356979098171\n",
            "Epoch 623 and loss: 0.01532586570829153\n",
            "Epoch 624 and loss: 0.0882614441215992\n",
            "Epoch 625 and loss: 0.08964522741734982\n",
            "Epoch 626 and loss: 0.014109227806329727\n",
            "Epoch 627 and loss: 0.012884320225566626\n",
            "Epoch 628 and loss: 0.02048262534663081\n",
            "Epoch 629 and loss: 0.01120963809080422\n",
            "Epoch 630 and loss: 0.00650279107503593\n",
            "Epoch 631 and loss: 0.0050208293832838535\n",
            "Epoch 632 and loss: 0.020218427293002605\n",
            "Epoch 633 and loss: 0.005694904015399516\n",
            "Epoch 634 and loss: 0.005485548113938421\n",
            "Epoch 635 and loss: 0.007443238282576203\n",
            "Epoch 636 and loss: 0.04367869911948219\n",
            "Epoch 637 and loss: 0.055557026993483305\n",
            "Epoch 638 and loss: 0.010808864142745733\n",
            "Epoch 639 and loss: 0.006264057126827538\n",
            "Epoch 640 and loss: 0.005105012969579548\n",
            "Epoch 641 and loss: 0.040572553407400846\n",
            "Epoch 642 and loss: 0.025703059509396553\n",
            "Epoch 643 and loss: 0.005278538563288748\n",
            "Epoch 644 and loss: 0.005428859731182456\n",
            "Epoch 645 and loss: 0.005258596152998507\n",
            "Epoch 646 and loss: 0.005504851462319493\n",
            "Epoch 647 and loss: 0.005975224310532212\n",
            "Epoch 648 and loss: 0.03926054330077022\n",
            "Epoch 649 and loss: 0.005739282816648483\n",
            "Epoch 650 and loss: 0.017031962634064257\n",
            "Epoch 651 and loss: 0.006719529163092375\n",
            "Epoch 652 and loss: 0.005474229750689119\n",
            "Epoch 653 and loss: 0.009270226350054145\n",
            "Epoch 654 and loss: 0.017769191414117813\n",
            "Epoch 655 and loss: 0.003305550490040332\n",
            "Epoch 656 and loss: 0.02136165980482474\n",
            "Epoch 657 and loss: 0.0068359519354999065\n",
            "Epoch 658 and loss: 0.00487126293592155\n",
            "Epoch 659 and loss: 0.030445174546912313\n",
            "Epoch 660 and loss: 0.007651010528206825\n",
            "Epoch 661 and loss: 0.10359470010735095\n",
            "Epoch 662 and loss: 0.018780122511088848\n",
            "Epoch 663 and loss: 0.12842423608526587\n",
            "Epoch 664 and loss: 0.023741371696814895\n",
            "Epoch 665 and loss: 0.016646023839712143\n",
            "Epoch 666 and loss: 0.008579037617892027\n",
            "Epoch 667 and loss: 0.009650920168496668\n",
            "Epoch 668 and loss: 0.030544782057404518\n",
            "Epoch 669 and loss: 0.015474233194254339\n",
            "Epoch 670 and loss: 0.061527286656200886\n",
            "Epoch 671 and loss: 0.00881209911312908\n",
            "Epoch 672 and loss: 0.0836828756146133\n",
            "Epoch 673 and loss: 0.017384326551109552\n",
            "Epoch 674 and loss: 0.007896254654042423\n",
            "Epoch 675 and loss: 0.011516699334606528\n",
            "Epoch 676 and loss: 0.007981661066878587\n",
            "Epoch 677 and loss: 0.016001981450244784\n",
            "Epoch 678 and loss: 0.00395694375038147\n",
            "Epoch 679 and loss: 0.01960890047485009\n",
            "Epoch 680 and loss: 0.0073413425125181675\n",
            "Epoch 681 and loss: 0.010111405747011304\n",
            "Epoch 682 and loss: 0.010042763780802488\n",
            "Epoch 683 and loss: 0.04702360928058624\n",
            "Epoch 684 and loss: 0.004812450148165226\n",
            "Epoch 685 and loss: 0.012471573427319527\n",
            "Epoch 686 and loss: 0.016512172762304544\n",
            "Epoch 687 and loss: 0.02185683767311275\n",
            "Epoch 688 and loss: 0.00926844822242856\n",
            "Epoch 689 and loss: 0.016199224861338735\n",
            "Epoch 690 and loss: 0.0546593654435128\n",
            "Epoch 691 and loss: 0.007817687059286982\n",
            "Epoch 692 and loss: 0.018590510357171297\n",
            "Epoch 693 and loss: 0.050036398926749825\n",
            "Epoch 694 and loss: 0.14742672338616103\n",
            "Epoch 695 and loss: 0.020324063254520297\n",
            "Epoch 696 and loss: 0.013223901856690645\n",
            "Epoch 697 and loss: 0.016321535338647664\n",
            "Epoch 698 and loss: 0.009985893499106169\n",
            "Epoch 699 and loss: 0.01296000462025404\n",
            "Epoch 700 and loss: 0.007504585257265717\n",
            "Epoch 701 and loss: 0.02358817565254867\n",
            "Epoch 702 and loss: 0.03030209767166525\n",
            "Epoch 703 and loss: 0.013049791101366282\n",
            "Epoch 704 and loss: 0.013548682327382267\n",
            "Epoch 705 and loss: 0.005486786365509033\n",
            "Epoch 706 and loss: 0.015560971631202847\n",
            "Epoch 707 and loss: 0.029294205480255187\n",
            "Epoch 708 and loss: 0.2668869385961443\n",
            "Epoch 709 and loss: 0.04327606549486518\n",
            "Epoch 710 and loss: 0.11781584657728672\n",
            "Epoch 711 and loss: 0.0196408205665648\n",
            "Epoch 712 and loss: 0.0348516833037138\n",
            "Epoch 713 and loss: 0.05570210120640695\n",
            "Epoch 714 and loss: 0.03341443766839802\n",
            "Epoch 715 and loss: 0.009385122801177204\n",
            "Epoch 716 and loss: 0.009553105803206563\n",
            "Epoch 717 and loss: 0.006968387635424733\n",
            "Epoch 718 and loss: 0.009598771925084293\n",
            "Epoch 719 and loss: 0.00810121581889689\n",
            "Epoch 720 and loss: 0.01941380789503455\n",
            "Epoch 721 and loss: 0.026159131317399442\n",
            "Epoch 722 and loss: 0.015104757505469024\n",
            "Epoch 723 and loss: 0.005306772538460791\n",
            "Epoch 724 and loss: 0.021706596249714494\n",
            "Epoch 725 and loss: 0.06086320662871003\n",
            "Epoch 726 and loss: 0.006693277042359114\n",
            "Epoch 727 and loss: 0.003675051382742822\n",
            "Epoch 728 and loss: 0.012818051327485591\n",
            "Epoch 729 and loss: 0.059997957199811935\n",
            "Epoch 730 and loss: 0.011515593156218529\n",
            "Epoch 731 and loss: 0.006368650123476982\n",
            "Epoch 732 and loss: 0.07901729573495686\n",
            "Epoch 733 and loss: 0.007312428788281977\n",
            "Epoch 734 and loss: 0.006057243444956839\n",
            "Epoch 735 and loss: 0.02008643001317978\n",
            "Epoch 736 and loss: 0.012865625554695725\n",
            "Epoch 737 and loss: 0.008524857345037162\n",
            "Epoch 738 and loss: 0.0034424111945554614\n",
            "Epoch 739 and loss: 0.04523741966113448\n",
            "Epoch 740 and loss: 0.009724163566716015\n",
            "Epoch 741 and loss: 0.007252935436554253\n",
            "Epoch 742 and loss: 0.0727756527485326\n",
            "Epoch 743 and loss: 0.007675227243453264\n",
            "Epoch 744 and loss: 0.013462441158480942\n",
            "Epoch 745 and loss: 0.00496325024869293\n",
            "Epoch 746 and loss: 0.01016118167899549\n",
            "Epoch 747 and loss: 0.033906217955518514\n",
            "Epoch 748 and loss: 0.010575134074315429\n",
            "Epoch 749 and loss: 0.011531616561114788\n",
            "Epoch 750 and loss: 0.00725474877981469\n",
            "Epoch 751 and loss: 0.0033854061039164662\n",
            "Epoch 752 and loss: 0.009548116358928382\n",
            "Epoch 753 and loss: 0.0027391259209252894\n",
            "Epoch 754 and loss: 0.0035706678172573447\n",
            "Epoch 755 and loss: 0.006203802535310388\n",
            "Epoch 756 and loss: 0.028081948403269053\n",
            "Epoch 757 and loss: 0.0022676139778923243\n",
            "Epoch 758 and loss: 0.003709569573402405\n",
            "Epoch 759 and loss: 0.00789953873027116\n",
            "Epoch 760 and loss: 0.02197583415545523\n",
            "Epoch 761 and loss: 0.03855176887009293\n",
            "Epoch 762 and loss: 0.00580881885252893\n",
            "Epoch 763 and loss: 0.010237404610961676\n",
            "Epoch 764 and loss: 0.004335785051807761\n",
            "Epoch 765 and loss: 0.0040519036701880395\n",
            "Epoch 766 and loss: 0.0038234500389080495\n",
            "Epoch 767 and loss: 0.003132775134872645\n",
            "Epoch 768 and loss: 0.047595142386853695\n",
            "Epoch 769 and loss: 0.00426281028194353\n",
            "Epoch 770 and loss: 0.0046698375372216105\n",
            "Epoch 771 and loss: 0.01225106863421388\n",
            "Epoch 772 and loss: 0.006099005229771137\n",
            "Epoch 773 and loss: 0.005880508106201887\n",
            "Epoch 774 and loss: 0.10610182746313512\n",
            "Epoch 775 and loss: 0.007340376148931682\n",
            "Epoch 776 and loss: 0.0052434137323871255\n",
            "Epoch 777 and loss: 0.01516765629639849\n",
            "Epoch 778 and loss: 0.01916477456688881\n",
            "Epoch 779 and loss: 0.003876218106597662\n",
            "Epoch 780 and loss: 0.0028665015124715865\n",
            "Epoch 781 and loss: 0.001941422204254195\n",
            "Epoch 782 and loss: 0.026630878739524633\n",
            "Epoch 783 and loss: 0.027311353012919426\n",
            "Epoch 784 and loss: 0.00908150477334857\n",
            "Epoch 785 and loss: 0.006770485546439886\n",
            "Epoch 786 and loss: 0.08293327130377293\n",
            "Epoch 787 and loss: 0.0073067983612418175\n",
            "Epoch 788 and loss: 0.010096607962623239\n",
            "Epoch 789 and loss: 0.053125783801078796\n",
            "Epoch 790 and loss: 0.03050741006154567\n",
            "Epoch 791 and loss: 0.0072645258624106646\n",
            "Epoch 792 and loss: 0.024418507353402674\n",
            "Epoch 793 and loss: 0.041565394727513194\n",
            "Epoch 794 and loss: 0.017811577068641782\n",
            "Epoch 795 and loss: 0.007858751341700554\n",
            "Epoch 796 and loss: 0.005000975856091827\n",
            "Epoch 797 and loss: 0.00501261162571609\n",
            "Epoch 798 and loss: 0.0026756420265883207\n",
            "Epoch 799 and loss: 0.022637882502749562\n",
            "Epoch 800 and loss: 0.0051406469428911805\n",
            "Epoch 801 and loss: 0.04167886031791568\n",
            "Epoch 802 and loss: 0.004615216690581292\n",
            "Epoch 803 and loss: 0.006945234606973827\n",
            "Epoch 804 and loss: 0.011717181070707738\n",
            "Epoch 805 and loss: 0.004302088404074311\n",
            "Epoch 806 and loss: 0.006167335493955761\n",
            "Epoch 807 and loss: 0.008508113794960082\n",
            "Epoch 808 and loss: 0.0087948044820223\n",
            "Epoch 809 and loss: 0.02331860808772035\n",
            "Epoch 810 and loss: 0.003971707541495562\n",
            "Epoch 811 and loss: 0.0168463466106914\n",
            "Epoch 812 and loss: 0.007774566387524828\n",
            "Epoch 813 and loss: 0.00632663955911994\n",
            "Epoch 814 and loss: 0.0031821614247746766\n",
            "Epoch 815 and loss: 0.008890529396012425\n",
            "Epoch 816 and loss: 0.007082171272486448\n",
            "Epoch 817 and loss: 0.015059051103889942\n",
            "Epoch 818 and loss: 0.007027838641079143\n",
            "Epoch 819 and loss: 0.023006698407698423\n",
            "Epoch 820 and loss: 0.025822915602475405\n",
            "Epoch 821 and loss: 0.004712919646408409\n",
            "Epoch 822 and loss: 0.003401055873837322\n",
            "Epoch 823 and loss: 0.009373112581670284\n",
            "Epoch 824 and loss: 0.008662633365020156\n",
            "Epoch 825 and loss: 0.0034686471335589886\n",
            "Epoch 826 and loss: 0.0055254974868148565\n",
            "Epoch 827 and loss: 0.03257783956360072\n",
            "Epoch 828 and loss: 0.0032011890434660017\n",
            "Epoch 829 and loss: 0.03230540466029197\n",
            "Epoch 830 and loss: 0.01877782741212286\n",
            "Epoch 831 and loss: 0.005961650924291462\n",
            "Epoch 832 and loss: 0.004189943079836667\n",
            "Epoch 833 and loss: 0.01258849073201418\n",
            "Epoch 834 and loss: 0.056545997969806194\n",
            "Epoch 835 and loss: 0.010995025921147317\n",
            "Epoch 836 and loss: 0.0071332992520183325\n",
            "Epoch 837 and loss: 0.003707323456183076\n",
            "Epoch 838 and loss: 0.006979690166190267\n",
            "Epoch 839 and loss: 0.004053697339259088\n",
            "Epoch 840 and loss: 0.002577275299699977\n",
            "Epoch 841 and loss: 0.004630571813322604\n",
            "Epoch 842 and loss: 0.0023376918979920447\n",
            "Epoch 843 and loss: 0.050827414088416845\n",
            "Epoch 844 and loss: 0.007626799109857529\n",
            "Epoch 845 and loss: 0.029385241214185953\n",
            "Epoch 846 and loss: 0.006147123698610812\n",
            "Epoch 847 and loss: 0.0031717456004116684\n",
            "Epoch 848 and loss: 0.004679908975958824\n",
            "Epoch 849 and loss: 0.0028956516180187464\n",
            "Epoch 850 and loss: 0.006339895422570407\n",
            "Epoch 851 and loss: 0.008696166682057083\n",
            "Epoch 852 and loss: 0.025519494898617268\n",
            "Epoch 853 and loss: 0.007250442577060312\n",
            "Epoch 854 and loss: 0.14145234948955476\n",
            "Epoch 855 and loss: 0.018568518105894327\n",
            "Epoch 856 and loss: 0.017732196138240397\n",
            "Epoch 857 and loss: 0.006406306871213019\n",
            "Epoch 858 and loss: 0.01191866584122181\n",
            "Epoch 859 and loss: 0.10459582856856287\n",
            "Epoch 860 and loss: 0.03447415726259351\n",
            "Epoch 861 and loss: 0.025900324108079076\n",
            "Epoch 862 and loss: 0.04137100698426366\n",
            "Epoch 863 and loss: 0.013098618481308222\n",
            "Epoch 864 and loss: 0.01341961300931871\n",
            "Epoch 865 and loss: 0.01109904085751623\n",
            "Epoch 866 and loss: 0.007178064435720444\n",
            "Epoch 867 and loss: 0.007526314118877053\n",
            "Epoch 868 and loss: 0.020011301152408123\n",
            "Epoch 869 and loss: 0.013586088782176375\n",
            "Epoch 870 and loss: 0.008514472981914878\n",
            "Epoch 871 and loss: 0.004734171088784933\n",
            "Epoch 872 and loss: 0.004999448196031153\n",
            "Epoch 873 and loss: 0.003576618735678494\n",
            "Epoch 874 and loss: 0.004100515507161617\n",
            "Epoch 875 and loss: 0.10610129544511437\n",
            "Epoch 876 and loss: 0.01030020450707525\n",
            "Epoch 877 and loss: 0.005964035401120782\n",
            "Epoch 878 and loss: 0.022821492049843073\n",
            "Epoch 879 and loss: 0.0149143747985363\n",
            "Epoch 880 and loss: 0.005404363153502345\n",
            "Epoch 881 and loss: 0.33758789114654064\n",
            "Epoch 882 and loss: 0.1278382232412696\n",
            "Epoch 883 and loss: 0.09035195969045162\n",
            "Epoch 884 and loss: 0.023048725444823503\n",
            "Epoch 885 and loss: 0.016019513830542564\n",
            "Epoch 886 and loss: 0.033039456931874156\n",
            "Epoch 887 and loss: 0.01776690687984228\n",
            "Epoch 888 and loss: 0.009936842252500355\n",
            "Epoch 889 and loss: 0.06633494049310684\n",
            "Epoch 890 and loss: 0.10681437281891704\n",
            "Epoch 891 and loss: 0.03422804409638047\n",
            "Epoch 892 and loss: 0.06464526313357055\n",
            "Epoch 893 and loss: 0.016761958599090576\n",
            "Epoch 894 and loss: 0.05184771050699055\n",
            "Epoch 895 and loss: 0.01746111421380192\n",
            "Epoch 896 and loss: 0.017902929335832596\n",
            "Epoch 897 and loss: 0.04203380178660154\n",
            "Epoch 898 and loss: 0.010422865161672235\n",
            "Epoch 899 and loss: 0.05294437101110816\n",
            "Epoch 900 and loss: 0.007110817241482437\n",
            "Epoch 901 and loss: 0.1375296557089314\n",
            "Epoch 902 and loss: 0.03807824710384011\n",
            "Epoch 903 and loss: 0.02295141899958253\n",
            "Epoch 904 and loss: 0.03559437347576022\n",
            "Epoch 905 and loss: 0.011104179546236992\n",
            "Epoch 906 and loss: 0.04248459776863456\n",
            "Epoch 907 and loss: 0.01117436052300036\n",
            "Epoch 908 and loss: 0.02693688264116645\n",
            "Epoch 909 and loss: 0.005506535526365042\n",
            "Epoch 910 and loss: 0.010288962628692389\n",
            "Epoch 911 and loss: 0.026318085147067904\n",
            "Epoch 912 and loss: 0.005186933267395943\n",
            "Epoch 913 and loss: 0.004563194583170116\n",
            "Epoch 914 and loss: 0.004383529536426067\n",
            "Epoch 915 and loss: 0.03305992693640292\n",
            "Epoch 916 and loss: 0.01534050703048706\n",
            "Epoch 917 and loss: 0.027264440432190895\n",
            "Epoch 918 and loss: 0.006506801582872868\n",
            "Epoch 919 and loss: 0.011389966122806072\n",
            "Epoch 920 and loss: 0.045572140254080296\n",
            "Epoch 921 and loss: 0.007748633157461882\n",
            "Epoch 922 and loss: 0.020490780705586076\n",
            "Epoch 923 and loss: 0.03346943750511855\n",
            "Epoch 924 and loss: 0.045886299572885036\n",
            "Epoch 925 and loss: 0.008373689313884825\n",
            "Epoch 926 and loss: 0.014422715408727527\n",
            "Epoch 927 and loss: 0.0065122428350150585\n",
            "Epoch 928 and loss: 0.004620361141860485\n",
            "Epoch 929 and loss: 0.006575729348696768\n",
            "Epoch 930 and loss: 0.011801829270552844\n",
            "Epoch 931 and loss: 0.017368762753903866\n",
            "Epoch 932 and loss: 0.01356177928391844\n",
            "Epoch 933 and loss: 0.008062903303653002\n",
            "Epoch 934 and loss: 0.027955326717346907\n",
            "Epoch 935 and loss: 0.004582340770866722\n",
            "Epoch 936 and loss: 0.06707540457136929\n",
            "Epoch 937 and loss: 0.17407064861617982\n",
            "Epoch 938 and loss: 0.03312290133908391\n",
            "Epoch 939 and loss: 0.027556809363886714\n",
            "Epoch 940 and loss: 0.008294400293380022\n",
            "Epoch 941 and loss: 0.009960890980437398\n",
            "Epoch 942 and loss: 0.051409687381237745\n",
            "Epoch 943 and loss: 0.1633820291608572\n",
            "Epoch 944 and loss: 0.01630726456642151\n",
            "Epoch 945 and loss: 0.014921613270416856\n",
            "Epoch 946 and loss: 0.026678861351683736\n",
            "Epoch 947 and loss: 0.0962134413421154\n",
            "Epoch 948 and loss: 0.013215057784691453\n",
            "Epoch 949 and loss: 0.028449094155803323\n",
            "Epoch 950 and loss: 0.01042347657494247\n",
            "Epoch 951 and loss: 0.008285012561827898\n",
            "Epoch 952 and loss: 0.3008818938396871\n",
            "Epoch 953 and loss: 0.06681731715798378\n",
            "Epoch 954 and loss: 0.016739005222916603\n",
            "Epoch 955 and loss: 0.027463508304208517\n",
            "Epoch 956 and loss: 0.014529065228998661\n",
            "Epoch 957 and loss: 0.04521420178934932\n",
            "Epoch 958 and loss: 0.02373427338898182\n",
            "Epoch 959 and loss: 0.09021230973303318\n",
            "Epoch 960 and loss: 0.12642577395308763\n",
            "Epoch 961 and loss: 0.016762818675488234\n",
            "Epoch 962 and loss: 0.04692353727295995\n",
            "Epoch 963 and loss: 0.09687377698719501\n",
            "Epoch 964 and loss: 0.01492801308631897\n",
            "Epoch 965 and loss: 0.018828405998647213\n",
            "Epoch 966 and loss: 0.007051660330034792\n",
            "Epoch 967 and loss: 0.025901007582433522\n",
            "Epoch 968 and loss: 0.030008249450474977\n",
            "Epoch 969 and loss: 0.006780774332582951\n",
            "Epoch 970 and loss: 0.009701270260848105\n",
            "Epoch 971 and loss: 0.010424683277960867\n",
            "Epoch 972 and loss: 0.02379176369868219\n",
            "Epoch 973 and loss: 0.08905551885254681\n",
            "Epoch 974 and loss: 0.01238260290119797\n",
            "Epoch 975 and loss: 0.01476687635295093\n",
            "Epoch 976 and loss: 0.097670090617612\n",
            "Epoch 977 and loss: 0.008548770798370242\n",
            "Epoch 978 and loss: 0.025421785307116807\n",
            "Epoch 979 and loss: 0.02612230263184756\n",
            "Epoch 980 and loss: 0.007888228399679065\n",
            "Epoch 981 and loss: 0.05337865091860294\n",
            "Epoch 982 and loss: 0.037492900621145964\n",
            "Epoch 983 and loss: 0.07130633434280753\n",
            "Epoch 984 and loss: 0.022987729869782925\n",
            "Epoch 985 and loss: 0.008423983817920089\n",
            "Epoch 986 and loss: 0.046703326515853405\n",
            "Epoch 987 and loss: 0.014738540165126324\n",
            "Epoch 988 and loss: 0.012131153605878353\n",
            "Epoch 989 and loss: 0.006863147835247219\n",
            "Epoch 990 and loss: 0.007272813003510237\n",
            "Epoch 991 and loss: 0.005366283934563398\n",
            "Epoch 992 and loss: 0.003162505803629756\n",
            "Epoch 993 and loss: 0.05792891315650195\n",
            "Epoch 994 and loss: 0.00505847274325788\n",
            "Epoch 995 and loss: 0.0032971339533105493\n",
            "Epoch 996 and loss: 0.0210712559055537\n",
            "Epoch 997 and loss: 0.020530504116322845\n",
            "Epoch 998 and loss: 0.00531213911017403\n",
            "Epoch 999 and loss: 0.05627534142695367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(resnet.state_dict(), './2000epochs-pretrained-net.pt')"
      ],
      "metadata": {
        "id": "mPHh5F1-hxEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "ecuAWIuRKyon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
        "resnet.load_state_dict(torch.load('./2000epochs-pretrained-net-4.pt'))\n",
        "resnet.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXeDLO7VTVRF",
        "outputId": "37f3499b-98d6-4cb1-bc26-892addd6ce88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('data/ball1.jpg')\n",
        "img = img.convert('RGB')\n",
        "\n",
        "input = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "resnet.eval()"
      ],
      "metadata": {
        "id": "toQgejlfmORW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bfa0136-2741-4673-f95a-45ca51d12bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "resnet.layer4.register_forward_hook(get_activation('attn'))\n",
        "resnet(input).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhOuXzbwLxkz",
        "outputId": "8e32b2a6-bfef-43e1-ff31-f8cb04e79fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "features = activation['attn'][0]\n",
        "print(features.shape)\n",
        "num_channels, num_x, num_y = features.shape\n",
        "features = features.reshape(num_channels, num_x * num_y)\n",
        "features = features.T.cpu()\n",
        "nn.functional.normalize(features, p = 2, dim = 1)\n",
        "print(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBMOgWhvL_mI",
        "outputId": "76fde18a-8fa8-47c4-aad3-38cf469028ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 8, 8])\n",
            "torch.Size([64, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "principal_components = pca.fit_transform(features)\n",
        "print(features.shape, principal_components.shape)"
      ],
      "metadata": {
        "id": "zPYk-pOyRlED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c1dc88-ce4d-4cf2-b4d3-6fcf4978984d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 512]) (64, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# from kneed import KneeLocator\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "pIk6femv28eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(\n",
        "    init=\"random\",\n",
        "    n_clusters=3,\n",
        "    n_init=10,\n",
        "    max_iter=1000,\n",
        "    random_state=42)"
      ],
      "metadata": {
        "id": "TCKdffV63HJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = kmeans.fit_predict(principal_components)\n",
        "print(labels.shape)\n",
        "labels = labels.reshape(1, num_x, num_y)\n",
        "print(torch.FloatTensor(labels).shape)\n",
        "\n",
        "from torch import nn\n",
        "labels = nn.functional.interpolate(torch.FloatTensor(labels).unsqueeze(0),\n",
        "                                   size=(256, 256), mode=\"nearest\")[0][0].cpu().numpy()\n",
        "# labels = labels[0]\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXpvhWaJ3JJZ",
        "outputId": "e217ace5-f5ea-4f05-bd53-d066558185ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64,)\n",
            "torch.Size([1, 8, 8])\n",
            "(256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "plt.imshow(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "3kkmG7Op3KzW",
        "outputId": "ac5131b8-a7d7-4b22-c3e9-de99ec5c29ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f680737d490>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAEOCAYAAACn/4O6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWcElEQVR4nO3df1DU953H8dcWI3JAXQZ21xh+eIFVfgwOIxY8GUWRYjgLxKqjjtNOaKgGOxMlEWE1M96kmS4CJZCWUDMksWNsYlmZq86lOM0IGhHYpoYyc1rGqSMqtbvL1vVYAmpg749cvr1tFVDe+wN4PWaYCd/vZ7/fz/eLebr7Xb6ryuFwuEBEJOQbvp4AEc0sjAoRiWJUiEgUo0JEohgVIhLFqBCRKEaFiER5PSqNjY1YunQpdDodMjMzcfHiRW9PgYg8yKtRaW5uRnl5OV599VWcP38eaWlp2LJlC27evOnNaRCRB6m8+Ru169atQ1JSEt566y1l2bJly1BQUIBDhw55axpE5EFee6Zy//59dHd3Iysry215VlYWurq6vDUNIvIwr0XFbrdjdHQUGo3GbblGo4HVavXWNIjIw/juDxGJmuOtHYWHhyMgIAA2m81tuc1mg1arfehjUo42emNqs15TRia2tJ/z9TRmhZlwrrtfKBp3vdeeqcydOxcpKSlobW11W97a2or09HRvTYOIPMxrz1QA4Ec/+hF27dqF1NRUpKen47333sNf//pXFBYWenMaRORBXo3Kd7/7Xfztb39DVVUVLBYLEhIS8Otf/xrR0dHenAYReZBXowIARUVFKCoa/zUZEU1ffPeHiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEsWoEJEoRoWIRDEqRCSKUSEiUYwKEYliVIhIFKNCRKIYFSISxagQkShGhYhEMSpEJIpRISJRjAoRiWJUiEiU1z9N31+9sv6/kDrvuq+n4RsDmTheUO/rWcwOHjjXfxhZhJozG0S3ORWMyv9JnXcdK+YF+HoaPnEJmLXH7m2eONej6BPd3lTx5Q8RiWJUiEgUo0JEohgVIhLFqBCRKEaFiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEiUSFaPRCLVa7fa1ePFiZb3L5YLRaER8fDwWLFiADRs24MqVKxK7JiI/I/ZMRa/Xo7e3V/m6ePGisq6urg719fU4fPgwzp49C41Gg40bN2JwcFBq90TkJ8SiMmfOHOh0OuUrIiICwFfPUhoaGrB3714UFBQgMTERDQ0NcDqdMJlMUrsnIj8hFpXr168jPj4eS5cuxQ9+8ANcv34dANDX1weLxYKsrCxlbFBQEFauXImuri6p3RORnxD5kKbly5fj7bffhl6vx8DAAKqqqpCTk4POzk5YLBYAgEajcXuMRqPB7du3x91uU0amxPQmZyATl7y3N79z6daHvp7CrCF9roMANGWIbnJKRKLy7W9/2+375cuXIyUlBb/61a/wrW9964m3u6X93FSnNmnHC+pn7aefXbr1IZZFbvf1NGYFT5zr9pExfP83u0W3OZ5uvX7c9R55SzkkJATx8fG4du0adDodAMBms7mNsdls0Gq1ntg9EfmQR6IyMjKCq1evQqfTISYmBjqdDq2trW7rOzo6kJ6e7ondE5EPibz8ee211/Dcc88hMjJSuabyxRdfYPv27VCpVCguLkZNTQ30ej3i4uJQXV2N4OBgbN68WWL3RORHRKLyl7/8BUVFRbDb7YiIiMDy5cvxu9/9DtHR0QCAPXv2YHh4GKWlpXA4HEhNTUVzczNCQ0Mldk9EfkQkKu+9996461UqFQwGAwwGg8TuiMiP8d4fIhLFqBCRKEaFiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEsWoEJEoRoWIRDEqRCSKUSEiUYwKEYliVIhIFKNCRKIYFSISxagQkShGhYhEMSpEJIpRISJRjAoRiWJUiEgUo0JEohgVIhLFqBCRKEaFiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEjXH1xPwF38YWYRR9Hl1nzFzvkDknBCv7pM869aXTvR9+S+PXB8EoH1kTHSfl4b/VXR7UzWpqLS3t+NnP/sZ/vjHP+L27duor6/Hjh07lPUulwsVFRX45S9/CYfDgdTUVFRXVyMhIUEZ43A4sH//frS0tAAAnnvuOVRWVkKtVgsf0pOpObPB6/tc/W//jfejP/X6fslzDvT/O9o7Ex+5vikD+P5vdntxRt43qZc/Q0NDSExMREVFBYKCgv5pfV1dHerr63H48GGcPXsWGo0GGzduxODgoDKmqKgIPT09MJlMMJlM6Onpwa5du+SOhIj8wqSeqeTk5CAnJwcAsHu3e2VdLhcaGhqwd+9eFBQUAAAaGhqg1+thMplQWFiI3t5efPLJJ2hpaUFaWhoA4M0330Rubi6uXr0KvV4veUxE5ENTvlDb19cHi8WCrKwsZVlQUBBWrlyJrq4uAIDZbEZISAjS09OVMStWrEBwcLAyhohmhilHxWKxAAA0Go3bco1GA6vVCgCwWq0IDw+HSqVS1qtUKkRERChjiGhm8Ot3f5oyMn09BQ/LxKVb/nHR7tKtD309hRlhbwCwN2P8MTP9z/WUo6LT6QAANpsNUVFRynKbzQatVgsA0Gq1sNvtcLlcyrMVl8uFgYEBZczDbGk/N9Xp+TV/effn0q0PsSxyu6+nMSN8v2/1BO/+ZE77P9fdE1wDnfLLn5iYGOh0OrS2tirLRkZG0NHRoVxDSUtLg9PphNlsVsaYzWYMDQ25XWchoulvUs9UnE4nrl27BgAYGxvDrVu30NPTg7CwMERFRaG4uBg1NTXQ6/WIi4tDdXU1goODsXnzZgDAkiVLkJ2djZKSEtTW1gIASkpKsH79er7zQzTDTCoqn3/+OfLy8pTvjUYjjEYjtm/fjoaGBuzZswfDw8MoLS1VfvmtubkZoaGhymMaGxuxf/9+bNq0CQCQm5uLyspK4cMhIl+bVFRWrVoFh8PxyPUqlQoGgwEGg+GRY9RqNd55553HnyERTSu8oZCIRDEqRCSKUSEiUYwKEYliVIhIFKNCRKIYFSISxagQkShGhYhEMSpEJIpRISJRjAoRiWJUiEgUo0JEohgVIhLFqBCRKEaFiET59T/RQTTdjLl88Pe0y/u7HA+j4kPnO5IQ25Hk62mgKQOIPfGSr6dBT2ie7RuINF703g4Li8ZdzZc/RCSKUSEiUYwKEYliVIhIFKNCRKIYFSISxagQkShGhYhEMSpEJIpRISJRjAoRiWJUiEgUo0JEohgVIhLFqBCRKEaFiERNKirt7e3Ytm0bEhISoFarcfz4cbf1xcXFUKvVbl/Z2dluY+7du4fS0lI8++yzWLhwIbZt24b+/n65IyEivzCpqAwNDSExMREVFRUICgp66Jg1a9agt7dX+WpqanJbbzAYcPr0abz77rv4+OOPMTg4iK1bt2J0dHTqR0FEfmNSHyeZk5ODnJwcAMDu3bsfOiYwMBA6ne6h6+7evYtjx46hvr4ea9euBQAcOXIEycnJaGtrw7p1655k7kTkh8SuqXR0dCAuLg6pqal4+eWXYbPZlHXd3d148OABsrKylGWRkZFYsmQJurq6pKZARH5A5IOvs7OzkZeXh5iYGNy4cQNvvPEG8vPz0dbWhsDAQFitVgQEBCA8PNztcRqNBlar9ZHbbcrIlJgeTQLPtfd45FxvXCW/zSckEpVNmzYp/52UlISUlBQkJyfjzJkzyM/Pf+Ltbmk/JzE9mkBTRibPtZd44lzPs3r30/RNdxrHXe+Rt5SffvppLFy4ENeuXQMAaLVajI6Owm63u42z2WzQarWemAIR+YhHomK323H79m3lwm1KSgqeeuoptLa2KmP6+/vR29uL9PR0T0yBiHxkUi9/nE6n8qxjbGwMt27dQk9PD8LCwhAWFoaKigrk5+dDp9Phxo0beP3116HRaPCd73wHADB//nx873vfw6FDh6DRaBAWFoaDBw8iKSkJa9as8djBEZH3TSoqn3/+OfLy8pTvjUYjjEYjtm/fjpqaGly+fBkfffQR7t69C51Oh1WrVuH9999HaGio22MCAgJQWFiIkZERrF69Gr/4xS8QEBAgf1RE5DOTisqqVavgcDgeub65uXnCbQQGBqKqqgpVVVWTnx0RTTu894eIRPEfaPchzWdA+AU/uP8pA4iv9YN5zAaeONf3H+BL2S1OCaPiQ3OHxvBl301fTwMA/GYes8FMP9d8+UNEohgVIhLFqBCRKEaFiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEsWoEJEoRoWIRDEqRCSKUSEiUYwKEYliVIhIFKNCRKIYFSISxagQkShGhYhEMSpEJIpRISJRjAoRiWJUiEgUo0JEohgVIhLFqBCRKEaFiEQxKkQkilEhIlGMChGJmjAqNTU1WLt2LaKiohAbG4utW7fi8uXLbmNcLheMRiPi4+OxYMECbNiwAVeuXHEb43A4sHPnTkRHRyM6Oho7d+6Ew+GQPRoi8rkJo3LhwgW8+OKLOHPmDE6dOoU5c+bg+eefx507d5QxdXV1qK+vx+HDh3H27FloNBps3LgRg4ODypiioiL09PTAZDLBZDKhp6cHu3bt8sxREZHPzJloQHNzs9v3R44cQXR0NDo7O5GbmwuXy4WGhgbs3bsXBQUFAICGhgbo9XqYTCYUFhait7cXn3zyCVpaWpCWlgYAePPNN5Gbm4urV69Cr9d74NCIyBce+5qK0+nE2NgY1Go1AKCvrw8WiwVZWVnKmKCgIKxcuRJdXV0AALPZjJCQEKSnpytjVqxYgeDgYGUMEc0Mjx2V8vJyJCcnK884LBYLAECj0biN02g0sFqtAACr1Yrw8HCoVCplvUqlQkREhDKGiGaGCV/+/H8HDhxAZ2cnWlpaEBAQ4Kk5KZoyMj2+D5/KAHBgra9nAQAwmst8PYVZY6af60lHxWAwoLm5GadPn8aiRYuU5TqdDgBgs9kQFRWlLLfZbNBqtQAArVYLu90Ol8ulPFtxuVwYGBhQxjzMlvZzj3Uw080z58YQ9J9mX08DRnMZDGmHfT2NWWEmnGvTncZx10/q5U9ZWRlOnjyJU6dOYfHixW7rYmJioNPp0NraqiwbGRlBR0eHcg0lLS0NTqcTZvPf/wcym80YGhpyu85CRNPfhM9U9u3bhxMnTuCDDz6AWq1WrqEEBwcjJCQEKpUKxcXFqKmpgV6vR1xcHKqrqxEcHIzNmzcDAJYsWYLs7GyUlJSgtrYWAFBSUoL169fznR+iGWbCqDQ2fvVU5+u3i79WVlYGg8EAANizZw+Gh4dRWloKh8OB1NRUNDc3IzQ01G07+/fvx6ZNmwAAubm5qKysFDsQIvIPE0ZlMr/1qlKpYDAYlMg8jFqtxjvvvPN4syOiaYf3/hCRKEaFiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEsWoEJEoRoWIRDEqRCSKUSEiUYwKEYliVIhI1GN9Rq23zbPO7OY99T8PfD0FInF+HZVI40VfT4GIHtPMfipARF7HqBCRKEaFiEQxKkQkilEhIlGMChGJYlSISBSjQkSiGBUiEsWoEJEoRoWIRDEqRCSKUSEiUYwKEYliVIhIFKNCRKIYFSISxagQkShGhYhETRiVmpoarF27FlFRUYiNjcXWrVtx+fJltzHFxcVQq9VuX9nZ2W5j7t27h9LSUjz77LNYuHAhtm3bhv7+ftmjISKfmzAqFy5cwIsvvogzZ87g1KlTmDNnDp5//nncuXPHbdyaNWvQ29urfDU1NbmtNxgMOH36NN599118/PHHGBwcxNatWzE6Oip7RETkUxN+mn5zc7Pb90eOHEF0dDQ6OzuRm5urLA8MDIROp3voNu7evYtjx46hvr4ea9euVbaTnJyMtrY2rFu3birHQER+5LGvqTidToyNjUGtVrst7+joQFxcHFJTU/Hyyy/DZrMp67q7u/HgwQNkZWUpyyIjI7FkyRJ0dXVNYfpE5G9UDofD9TgPeOGFF/DnP/8ZbW1tCAgIAACcPHkSQUFBiImJwY0bN/DGG29gbGwMbW1tCAwMRFNTE1566SUMDAxApVIp28rLy0NsbCxqa2sfuq+rV69O4dCIyBP0ev246x/rHxM7cOAAOjs70dLSogQFADZt2qT8d1JSElJSUpCcnIwzZ84gPz//Maf8d4a0w0/8WJo8o7mM59pLZsK5Nt1pHHf9pF/+GAwGnDx5EqdOncKiRYvGHfv0009j4cKFuHbtGgBAq9VidHQUdrvdbZzNZoNWq53sFIhoGphUVMrKypSgLF68eMLxdrsdt2/fVi7cpqSk4KmnnkJra6sypr+/H729vUhPT3/CqRORP5rwmsq+fftw4sQJfPDBB4iPj1eWBwcHIyQkBE6nExUVFcjPz4dOp8ONGzfw+uuvo7+/H11dXQgNDQUAvPLKK2hpacHbb7+NsLAwHDx4EA6HA+fOnXN7KUVE09uEUfnHd3m+VlZWBoPBgOHhYezYsQM9PT24e/cudDodVq1ahYMHDyIyMlIZf+/ePbz22mswmUwYGRnB6tWr8dOf/tRtDBFNf4/97g8R0Xh47w8RiWJUiEgUo0JEovwuKo2NjVi6dCl0Oh0yMzNx8eJFX09p2mlvb8e2bduQkJAAtVqN48ePu613uVwwGo2Ij4/HggULsGHDBly5csVtjMPhwM6dOxEdHY3o6Gjs3LkTDofDm4cxLUzmLv7Zdr79KirNzc0oLy/Hq6++ivPnzyMtLQ1btmzBzZs3fT21aWVoaAiJiYmoqKhAUFDQP62vq6tDfX09Dh8+jLNnz0Kj0WDjxo0YHBxUxhQVFaGnpwcmkwkmkwk9PT3YtWuXNw9jWpjMXfyz7Xz71bs/69atQ1JSEt566y1l2bJly1BQUIBDhw75cGbT1zPPPIPKykrs2LEDwFd/a8bHx+OHP/wh9u3bBwAYHh6GXq/Hj3/8YxQWFiq/lNjS0oIVK1YA+OqG0dzcXPz+97+f8N6P2czpdCI6OhrHjx9Hbm7urDzffvNM5f79++ju7na7kxkAsrKyeCezoL6+PlgsFrfzHBQUhJUrVyrn2Ww2IyQkxO23nVesWIHg4GD+LCbwj3fxz8bz7TdRsdvtGB0dhUajcVuu0WhgtVp9NKuZx2KxAMC459lqtSI8PNztjnKVSoWIiAj+LCZQXl6O5ORkpKWlAZid5/ux7lImokd71F38s43fPFMJDw9HQECA24c7AbyTWdrXN3mOd561Wi3sdjtcrr9fbnO5XBgYGODP4hEedRf/bDzffhOVuXPnIiUlxe1OZgBobW3lncyCYmJioNPp3M7zyMgIOjo6lPOclpYGp9MJs9msjDGbzRgaGuLP4iHGu4t/Np7vgPLy8v/w9SS+FhoaCqPRiAULFmDevHmoqqrCxYsX8fOf/xzz58/39fSmDafTiT/96U+wWCw4duwYEhMT8c1vfhP379/H/PnzMTo6itraWsTGxmJ0dBQHDx6ExWJBbW0tAgMDERERgc8++wwmkwnJycno7+9HSUkJli1bNm3f5vSUffv24aOPPsLRo0cRGRmJoaEhDA0NAfjqL0qVSjXrzrdfvaUMfPXLb3V1dbBYLEhISMBPfvITZGRk+Hpa08qnn36KvLy8f1q+fft2NDQ0wOVyoaKiAkePHoXD4UBqaiqqq6uRmJiojHU4HNi/fz9++9vfAgByc3NRWVn5yLvWZ6uJ7uIHMOvOt99FhYimN7+5pkJEMwOjQkSiGBUiEsWoEJEoRoWIRDEqRCSKUSEiUYwKEYliVIhI1P8C6fo6jJ3zO+EAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AzDueivF3ZB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}